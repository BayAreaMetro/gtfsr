
---
title: "Getting GTFS Data and Mapping with gtfsr"
author: "Danton Noriega <danton@transloc.com>"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using gtfsr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r init, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(cache=TRUE, tidy=TRUE, collapse = TRUE, comment = "#>")
```
## 1. Get an GTFS API key

This package can get data from a user-specified URL and is also able to get GTFS data from the [TransitFeeds API](http://transitfeeds.com/api/). This vignette will focus on the case where GTFS data is extracted from the TransitFeed API. Below are the steps needed to get a API key (note: requires a GitHub account), including a YouTube (click the GIF to see the YouTube video) that visually guides you through the steps.

1. *Go to [http://transitfeeds.com/](http://transitfeeds.com/)*
2. *Click* "Sign in with GitHub" *in the top-right corner.*
	- If it is your first time visiting the site, it will ask you to sign in (and likely every time if you do not have cookies enabled).
3. *Once signed in, click your profile icon in the top-right and select* "API Keys" *from the drop-down menu.*
	- Your GitHub profile icon and username replaces "Sign in with GitHub".
4. *Fill in* "Enter a description" *and then click the* "Create Key" *button*.
5. *Copy your new API Key to your clipboard.*

[![vid-gif](https://j.gifs.com/kRNVY5.gif)](https://youtu.be/ufM67FoIMho)



## 2. Use `gtfsr` package to download feed list

First things first, load the `gtfsr` package and set your key to access the TransitFeeds API. This example also using the `dplyr` package to manage data frames and `magrittr` for piping.

```{r setup, warning=TRUE, message=FALSE, echo=TRUE, eval=TRUE}

library(gtfsr)
library(magrittr)
library(dplyr)
options(dplyr.width = Inf) # I like to see all the columns

set_api_key('2ec1ae29-b8c2-4a03-b96e-126d585233f9') # input your API key here

```

### Getting full list available GTFS feeds

With a valid API key loaded, you can easily get the full list of GTFS feeds using the `get_feedlist` function. What we care most about are the feed GTFS data urls contained in column `url_d` of the feed list. Since we are interested in acquiring the GTFS data (not just the feedlist), we can use the `filter_feedlist` function to return a data frame containing only valid feed urls.

```{r feedlist, warning=TRUE, message=TRUE, echo=TRUE, eval=TRUE}

feedlist_df <- get_feedlist() # create a data frame of all feeds
feedlist_df <- feedlist_df %>% filter_feedlist # filter the feedlist
feedlist_df %>% select(url_d) %>% head(5) # show first 5 feed urls

```

If we want only the data for a specific location (or locations), we can get then search the feedlist for feeds of interest.

Assume we are interested in getting all the GTFS data from *Australian* feeds (i.e. we search for location names for the word 'australia'). We can match Australian agencies by name (filter on `loc_t`) and extract the corresponding url feeds (select `url_d`).


### Subsetting the GTFS feedlist (Example)


```{r aussie, warning=TRUE, message=TRUE, echo=TRUE, eval=TRUE}

## get australian feeds
aussie_df <- feedlist_df %>%
    filter(grepl('australia', loc_t, ignore.case = TRUE)) # filter out locations with "australia" in name
aussie_df %>% select(loc_t) %>% head(5) # look at location names
aussie_urls <- aussie_df %>% select(url_d) # get aussie urls

```

Once we have the urls for the feeds of interest, we can download and extract all the GTFS data into a data list (aka a list of data frames) using the `import_gtfs` data. To save time, we will only download 2 feeds (`slice(8:9)`). The output is suppressed because of how verbose it is.

```{r import_gtfs, warning=FALSE, message=FALSE, echo=TRUE, eval=FALSE}

data_list <- aussie_urls %>% slice(8:9) %>% import_gtfs

```

### Inspect Parsing Warnings During Process

During the import of the any feed url, you will see the following message:

```
NOTE: Parsing errors and warnings while importing data can be extracted from any given data frame with `attr(df, "problems")`.
```

This output was suppressed in the last section to save space given how verbose it is. But the highlighted `NOTE` explains that *if one observes an error or warning during the import process*, one can extract a data frame of problems, which is stored as an attribute for any data frame that had a warning output.

As an example, let's extract the data and problems data frames for the 44th url of `feedlist_df`.

```{r problems, warning=FALSE, message=FALSE, echo=TRUE, eval=TRUE, results='hide'}
url <- feedlist_df %>% slice(44) %>% select(url_d)
data_list1 <- url %>% import_gtfs
```

If you look at the output created when creating the `data_list1` object, you should see output that looks like this...

```
...
Warning: 2 parsing failures.
row col   expected    actual
  3  -- 10 columns 1 columns
  4  -- 10 columns 1 columns
...
```

To understand the problem, let's extract the data frame `calendar_df`. Recall that `import_gtfs` returns a list where each element of the list is a set of data frames.


```{r title, echo=TRUE, eval=TRUE}
df <- data_list1[[1]]$calendar_df # extract `calendar_df` from the 1st list element (there's only one)
df
attr(df, 'problems')

```

From inspecting the output from `attr(df, 'problems')` and comparing it to just `df`, it appears the problems (at least for this particular `calendar_df`) stem from the possibility that empty rows were added to the end of the original text file. Not a big deal and easily fixed. But we leave such specific fixes to the user to correct.








