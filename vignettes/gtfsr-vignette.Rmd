
---
title: "Getting GTFS Data and Mapping with gtfsr"
author: "Danton Noriega <danton@transloc.com>"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using gtfsr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Getting data from GTFS Transit Feeds

### 1. Get an GTFS API key

This package can get data from a user-specified URL and is also able to get GTFS data from the [TransitFeeds API](http://transitfeeds.com/api/). This vignette will focus on the case where GTFS data is extracted from the TransitFeed API. Below are the steps needed to get a API key (note: requires a GitHub account), including a YouTube (click the GIF to see the YouTube video) that visually guides you through the steps.

1. *Go to [http://transitfeeds.com/](http://transitfeeds.com/)*
2. *Click* "Sign in with GitHub" *in the top-right corner.*
	- If it is your first time visiting the site, it will ask you to sign in (and likely every time if you do not have cookies enabled).
3. *Once signed in, click your profile icon in the top-right and select* "API Keys" *from the drop-down menu.*
	- Your GitHub profile icon and username replaces "Sign in with GitHub".
4. *Fill in* "Enter a description" *and then click the* "Create Key" *button*.
5. *Copy your new API Key to your clipboard.*

[![vid-gif](https://j.gifs.com/kRNVY5.gif)](https://youtu.be/ufM67FoIMho)



### 2. Use `gtfsr` package to download feed list

First things first, load the `gtfsr` package and set your key to access the TransitFeeds API. This example also using the `dplyr` package to manage dataframes and `magrittr` for piping.

```{r setup, warning=TRUE, message=FALSE, echo=TRUE, eval=TRUE}

library(gtfsr)
library(magrittr)
library(dplyr)
options(dplyr.width = Inf) # I like to see all the columns

set_api_key('2ec1ae29-b8c2-4a03-b96e-126d585233f9') # input your API key here

```

### Getting full list available GTFS feeds

With a valid API key loaded, you can easily get the full list of GTFS feeds using the `get_feedlist` function. What we care most about are the feed GTFS data urls contained in column `url_d` of the feed list. Since we are interested in acquiring the GTFS data (not just the feedlist), we can use the `filter_feedlist` function to return a dataframe containing only valid feed urls.

```{r feedlist, warning=TRUE, message=TRUE, echo=TRUE, eval=TRUE}

feedlist_df <- get_feedlist() # create a dataframe of all feeds
feedlist_df <- feedlist_df %>% filter_feedlist # filter the feedlist
feedlist_df %>% select(url_d) %>% head(5) # show first 5 feed urls

```

If we want only the data for a specific location (or locations), we can get then search the feedlist for feeds of interest.

Assume we are interested in getting all the GTFS data from *Australian* feeds (i.e. we search for location names for the word 'australia'). We can match Australian agencies by name (filter on `loc_t`) and extract the corresponding url feeds (select `url_d`).




```{r aussie, warning=TRUE, message=TRUE, echo=TRUE, eval=TRUE}

## get australian feeds
aussie_df <- feedlist_df %>%
    filter(grepl('australia', loc_t, ignore.case = TRUE)) # filter out locations with "australia" in name
aussie_df %>% select(loc_t) %>% head(5) # look at location names
aussie_urls <- aussie_df %>% select(url_d) # get aussie urls

```

Once we have the urls for the feeds of interest, we can download and extract all the GTFS data into a data list (aka a list of data frames) using the `get_gtfs` data. To save time, we will only download 5 feeds (`slice(6:10)`)

```{r get_gtfs, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

data_list <- aussie_urls %>% slice(6:10) %>% get_gtfs

```


